var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"AWS Data Pipeline","text":""},{"location":"index.html#requirements","title":"Requirements","text":"<ul> <li>Parquet files arrive in a S3 bucket every 10 minutes</li> <li>Has fields, id, timestamp and multi level nested JSON data</li> <li>Json field needs to be processed</li> <li>Need Processing state tracking and error handling</li> <li>Auto-scaling</li> <li>Monitoring</li> </ul> <p>Click on a tile to change the primary color:</p> <code>red</code> <code>pink</code> <code>purple</code> <code>deep purple</code> <code>indigo</code> <code>blue</code> <code>light blue</code> <code>cyan</code> <code>teal</code> <code>green</code> <code>light green</code> <code>lime</code> <code>yellow</code> <code>amber</code> <code>orange</code> <code>deep orange</code> <code>brown</code> <code>grey</code> <code>blue grey</code> <code>black</code> <code>white</code>"},{"location":"Architecture/architecture.html","title":"Overview","text":"<p>Parquet files are written every 10 minutes to S3. We will review the options for a scalable, reliable and event driven architecture on AWS. Workflow can optionally be driven by step functions.(Step functions are charged based on number of state transitions)</p>"},{"location":"Architecture/architecture.html#aws-lambda","title":"AWS Lambda","text":"<p>S3 \u2192 EventBridge/SQS \u2192 Lambda \u2192 DynamoDB (state) \u2192 Target System</p>"},{"location":"Architecture/architecture.html#aws-glue","title":"AWS Glue","text":"<p>S3 \u2192 Glue Crawler \u2192 Glue Catalog \u2192 Glue Job \u2192 Processed Data</p>"},{"location":"Architecture/architecture.html#aws-athena","title":"AWS Athena","text":"<p>S3 \u2192 Athena \u2192 Target System</p>"},{"location":"Architecture/athena.html","title":"AWS Athena","text":"<p>It is included here for completeness.</p>"},{"location":"Architecture/athena.html#features","title":"Features","text":"<ul> <li>Query-based: SQL interface for JSON processing</li> <li>Serverless: No infrastructure management</li> <li>Cost-effective: Pay per query</li> <li>JSON functions: Built-in JSON parsing capabilities</li> </ul>"},{"location":"Architecture/athena.html#limitations","title":"Limitations","text":"<ul> <li>Athena is query-oriented, not processing-oriented.</li> <li>No built-in scheduling or event-driven processing</li> </ul>"},{"location":"Architecture/awslambda.html","title":"AWS Lambda","text":""},{"location":"Architecture/awslambda.html#workflow","title":"Workflow","text":"<ul> <li>Trigger a Lambda function when files arrive in S3 bucket</li> <li>Read and process parquet file using pyarrow/fastparquet</li> <li>write status and error logs     <ul> <li>Track status in DynamoDB. Both success and failure</li> <li>Use Dead letter Queue SQS</li> </ul> </li> <li>Step Functions for orchesteration(Optional)</li> </ul>"},{"location":"Architecture/awslambda.html#pros","title":"Pros","text":"<ul> <li>Event-driven, processes files immediately</li> <li>Automatic scaling with concurrent executions</li> <li>Cost-effective for sporadic workloads</li> <li>Low operational overhead</li> </ul>"},{"location":"Architecture/awslambda.html#cons","title":"Cons","text":"<ul> <li>15-minute timeout limit</li> <li>10GB memory limit</li> <li>512MB ephemeral storage (can extend to 10GB)</li> <li>Cold start latency unless provisioned</li> <li>Step functions cost is based on number of transitions</li> </ul>"},{"location":"Architecture/awslambda.html#decoupling","title":"Decoupling","text":"<p>Use SQS for a decoupled design. S3 triggers an Eventbridge rule with SQS as target. SQS has builtin retry logic and backpressure tolerance.</p>"},{"location":"Architecture/glue.html","title":"AWS Glue","text":"<p>AWS Glue is more suited for ETL and Integration. It can run in Python Shell mode or Spark. A Glue Job can be triggered using EventBridge.</p>"},{"location":"Architecture/glue.html#features","title":"Features","text":"<ul> <li>Glue Crawler to catalog data</li> <li>Glue ETL jobs (Spark-based)</li> <li>Bookmarking for tracking processed files</li> <li>Glue Data Quality for validation</li> <li>CloudWatch for monitoring</li> </ul>"},{"location":"Architecture/glue.html#pros","title":"Pros","text":"<ul> <li>Built-in bookmarking for state tracking</li> <li>Handles large files efficiently</li> <li>Auto-scaling with DPUs</li> <li>Rich ETL capabilities</li> <li>Supports streaming with Glue Streaming</li> </ul>"},{"location":"Architecture/glue.html#cons","title":"Cons:","text":"<ul> <li>Higher minimum cost (10-minute billing minimum)</li> <li>Spark overhead for small files</li> <li>More complex setup</li> </ul>"},{"location":"Architecture/monitoring.html","title":"Monitoring","text":"<ul> <li>Cloudwatch Logs for Lambda function</li> <li>Cloudwatch Metrics and Alarms</li> <li>DynamoDB streams for realtime tracking</li> </ul>"},{"location":"Implementation/ErrorHandling.html","title":"Error Handling","text":"<p>Transient errors are handled by a retry.</p> <p>If Lambda fails after max retries (default is 2), the event is sent to a DLQ (SQS).</p>"},{"location":"Implementation/ErrorHandling.html#workflow","title":"Workflow","text":"<p>Lambda processes Parquet files from S3.</p> <p>If Lambda fails after max retries (default is 2), the event is sent to a DLQ (SQS).</p> <p>You can inspect or reprocess messages in the DLQ manually or automatically.</p>"},{"location":"Implementation/ErrorHandling.html#setup","title":"Setup","text":"<ol> <li> <p>Create an SQS Queue for DLQ You can do this via the AWS Console or CLI: aws sqs create-queue --queue-name parquet-processing-dlq</p> </li> <li> <p>Attach DLQ to Lambda If using the AWS CLI:</p> </li> </ol> <p>aws lambda update-function-configuration \\   --function-name parquetProcessor \\   --dead-letter-config TargetArn=arn:aws:sqs:us-east-1:123456789012:parquet-processing-dlq If using the AWS Console:</p> <p>Go to your Lambda function</p> <p>Configuration \u2192 Asynchronous Invocation \u2192 DLQ</p> <p>Select the SQS queue</p> <p>Make sure the Lambda function has sqs:SendMessage permission for this DLQ.</p> <ol> <li>IAM Policy for Lambda to Send to DLQ Attach this IAM permission to the Lambda role:</li> </ol> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": \"sqs:SendMessage\",\n  \"Resource\": \"arn:aws:sqs:us-east-1:123456789012:parquet-processing-dlq\"\n}\n</code></pre> <p>Sample Python Lambda Code with Error</p> <pre><code>import boto3\nimport pyarrow.parquet as pq\nimport io\n\ndef lambda_handler(event, context):\n    s3 = boto3.client('s3')\n\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = record['s3']['object']['key']\n\n        try:\n            obj = s3.get_object(Bucket=bucket, Key=key)\n            table = pq.read_table(source=io.BytesIO(obj['Body'].read()))\n            # Process table...\n            print(f\"Processed file: {key}\")\n        except Exception as e:\n            print(f\"Error processing file {key}: {str(e)}\")\n            raise e  # Rethrow to trigger retry and potentially DLQ\n</code></pre> <ol> <li>View Messages in DLQ (Optional Reprocessing)</li> </ol> <pre><code>aws sqs receive-message --queue-url https://sqs.us-east-1.amazonaws.com/123456789012/parquet-processing-dlq\n</code></pre> <p>\ud83d\udd0d Notes DLQ only captures asynchronous invocations (e.g., from S3, SQS, EventBridge). DLQ stores the original event payload that failed.</p>"},{"location":"Implementation/JSONProcessing.html","title":"JSON Processing","text":"<p>For processing JSON, there are 2 options. Streaming and Full load.</p> <p>Python ijson uses streaming.</p> <p>If JSON is present as a column in a parquet file, it can be treated as line-delimited json. Memory may not be an issue in such a case.</p> <p>pyarrow can be considered for reading parquet and its json field.</p>"},{"location":"Implementation/JSONProcessing.html#py-arrow-parquet","title":"py arrow parquet","text":"<pre><code>import ijson\nimport pyarrow.parquet as pq\n\ndef process_parquet_streaming(s3_key):\n    # Read parquet file\n    table = pq.read_table(f's3://{bucket}/{s3_key}')\n\n    # Process each row's JSON column\n    for batch in table.to_batches():\n        for row in batch.to_pandas().itertuples():\n            # Stream parse JSON without loading full structure\n            json_data = row.metadata_column\n\n            # Use ijson for streaming parsing\n            parser = ijson.parse(json_data)\n            for prefix, event, value in parser:\n                # Process JSON incrementally\n                process_json_element(prefix, event, value, row.id, row.timestamp)\n</code></pre>"},{"location":"Implementation/JSONProcessing.html#nested-json","title":"Nested JSON","text":"<pre><code>import ijson\nimport io\nimport json\nfrom typing import Dict, Any\n\ndef parse_nested_json_streaming(parquet_path: str, json_column: str):\n    \"\"\"\n    Parse a Parquet file with streaming JSON parsing for a nested JSON column.\n\n    Args:\n        parquet_path: Path to the Parquet file\n        json_column: Name of the column containing nested JSON data\n    \"\"\"\n    # Open the Parquet file\n    parquet_file = pq.ParquetFile(parquet_path)\n\n    # Get the batches (streaming approach)\n    for batch in parquet_file.iter_batches():\n        # Convert batch to pandas (for easy access)\n        df = batch.to_pandas()\n\n        # Process each row\n        for index, row in df.iterrows():\n            json_str = row[json_column]\n\n            # Skip null/empty values\n            if not json_str or pd.isna(json_str):\n                continue\n\n            # Create a file-like object for ijson\n            json_bytes = json_str.encode('utf-8') if isinstance(json_str, str) else json_str\n            json_io = io.BytesIO(json_bytes)\n\n            # Use ijson for streaming parse\n            try:\n                # Example: Parse the JSON stream for specific elements\n                parser = ijson.parse(json_io)\n\n                # This is where you'd implement your specific parsing logic\n                # Here's an example that just collects paths and values\n                for prefix, event, value in parser:\n                    print(f\"Path: {prefix}, Event: {event}, Value: {value}\")\n\n                # Alternative: Extract specific nested values\n                json_io.seek(0)  # Reset for new parse\n                nested_value = ijson.items(json_io, 'some.nested.path')\n                for item in nested_value:\n                    print(f\"Nested value: {item}\")\n\n            except Exception as e:\n                print(f\"Error parsing JSON in row {index}: {str(e)}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    parse_nested_json_streaming('data.parquet', 'json_data_column')\n</code></pre>"},{"location":"Implementation/LambdaHandler.html","title":"Lambda Handler","text":"<pre><code># Example Lambda function for small files\nimport json\nimport boto3\nimport pyarrow.parquet as pq\nfrom io import BytesIO\nimport ijson\n\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('processing-state')\n\ndef lambda_handler(event, context):\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = record['s3']['object']['key']\n\n        try:\n            # Stream Parquet file\n            s3 = boto3.client('s3')\n            obj = s3.get_object(Bucket=bucket, Key=key)\n\n            # Read Parquet in chunks\n            parquet_file = pq.ParquetFile(BytesIO(obj['Body'].read()))\n\n            for batch in parquet_file.iter_batches(batch_size=1000):\n                df = batch.to_pandas()\n\n                # Stream parse JSON column\n                for idx, row in df.iterrows():\n                    json_data = row['metadata']\n                    # Process nested JSON with minimal memory\n                    process_json_stream(json_data, row['id'])\n\n            # Mark as processed\n            table.put_item(Item={\n                'file_key': key,\n                'status': 'completed',\n                'timestamp': context.aws_request_id\n            })\n\n        except Exception as e:\n            # Log error and send to DLQ\n            table.put_item(Item={\n                'file_key': key,\n                'status': 'failed',\n                'error': str(e)\n            })\n            raise\n\ndef process_json_stream(json_str, record_id):\n    # Use ijson for streaming JSON parsing\n    parser = ijson.items(BytesIO(json_str.encode()), '')\n    for obj in parser:\n        # Process nested objects with minimal memory\n        pass\n</code></pre>"},{"location":"Implementation/dynamodb.html","title":"DynamoDB","text":"<p>Partition Key: file_path Attributes: processing_status, timestamp, error_message, retry_count</p>"},{"location":"Scripts/dlq.html","title":"Dlq","text":"<p>Here's a complete Terraform script that:</p> <p>Creates an S3 bucket (for Parquet uploads)</p> <p>Creates an SQS queue (as a Dead Letter Queue)</p> <p>Creates a Lambda function (that processes Parquet files)</p> <p>Sets up an S3 Event Notification to trigger the Lambda</p> <p>Configures DLQ on the Lambda</p> <p>variables.tf terraform/ \u2502 \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 variables.tf \u251c\u2500\u2500 lambda/ \u2502   \u2514\u2500\u2500 lambda_function.py</p> <p>variable \"region\" {   default = \"us-east-1\" }</p> <p>main.tf provider \"aws\" {   region = var.region }</p> <p>resource \"aws_s3_bucket\" \"parquet_bucket\" {   bucket = \"parquet-ingest-${random_id.suffix.hex}\" }</p> <p>resource \"random_id\" \"suffix\" {   byte_length = 4 }</p> <p>resource \"aws_sqs_queue\" \"dlq\" {   name = \"parquet-processing-dlq\" }</p> <p>resource \"aws_iam_role\" \"lambda_exec_role\" {   name = \"lambda_parquet_exec_role\"</p> <p>assume_role_policy = jsonencode({     Version = \"2012-10-17\",     Statement = [{       Action = \"sts:AssumeRole\",       Effect = \"Allow\",       Principal = {         Service = \"lambda.amazonaws.com\"       }     }]   }) }</p> <p>resource \"aws_iam_policy\" \"lambda_policy\" {   name = \"lambda_parquet_policy\"</p> <p>policy = jsonencode({     Version = \"2012-10-17\",     Statement = [       {         Effect = \"Allow\",         Action = [           \"s3:GetObject\",           \"s3:ListBucket\"         ],         Resource = [           aws_s3_bucket.parquet_bucket.arn,           \"${aws_s3_bucket.parquet_bucket.arn}/\"         ]       },       {         Effect = \"Allow\",         Action = \"logs:\",         Resource = \"*\"       },       {         Effect = \"Allow\",         Action = \"sqs:SendMessage\",         Resource = aws_sqs_queue.dlq.arn       }     ]   }) }</p> <p>resource \"aws_iam_role_policy_attachment\" \"attach_policy\" {   role       = aws_iam_role.lambda_exec_role.name   policy_arn = aws_iam_policy.lambda_policy.arn }</p> <p>data \"archive_file\" \"lambda_zip\" {   type        = \"zip\"   source_dir  = \"\\({path.module}/lambda\"   output_path = \"\\)/lambda.zip\" }</p> <p>resource \"aws_lambda_function\" \"parquet_processor\" {   function_name = \"parquetProcessor\"   filename      = data.archive_file.lambda_zip.output_path   source_code_hash = data.archive_file.lambda_zip.output_base64sha256   role          = aws_iam_role.lambda_exec_role.arn   handler       = \"lambda_function.lambda_handler\"   runtime       = \"python3.11\"   timeout       = 30</p> <p>dead_letter_config {     target_arn = aws_sqs_queue.dlq.arn   } }</p> <p>resource \"aws_s3_bucket_notification\" \"bucket_notify\" {   bucket = aws_s3_bucket.parquet_bucket.id</p> <p>lambda_function {     lambda_function_arn = aws_lambda_function.parquet_processor.arn     events              = [\"s3:ObjectCreated:*\"]   }</p> <p>depends_on = [aws_lambda_permission.allow_s3] }</p> <p>resource \"aws_lambda_permission\" \"allow_s3\" {   statement_id  = \"AllowExecutionFromS3\"   action        = \"lambda:InvokeFunction\"   function_name = aws_lambda_function.parquet_processor.function_name   principal     = \"s3.amazonaws.com\"   source_arn    = aws_s3_bucket.parquet_bucket.arn }</p> <p>Step 3: lambda/lambda_function.py import boto3 import pyarrow.parquet as pq import io</p> <p>def lambda_handler(event, context):     s3 = boto3.client('s3')     for record in event['Records']:         bucket = record['s3']['bucket']['name']         key = record['s3']['object']['key']         try:             obj = s3.get_object(Bucket=bucket, Key=key)             table = pq.read_table(source=io.BytesIO(obj['Body'].read()))             print(f\"Processed file: {key}\")         except Exception as e:             print(f\"ERROR processing {key}: {e}\")             raise e  # Ensure Lambda fails to trigger DLQ</p> <p>terraform init terraform apply</p> <p>You will get:</p> <p>An S3 bucket (name shown in output)</p> <p>An SQS queue as DLQ</p> <p>A Lambda that logs error and triggers DLQ if it fails</p> <p>Automatic trigger from S3 when a file is uploaded</p> <p>Would you like this extended to include:</p> <p>DynamoDB for tracking processed files?</p> <p>Error dashboard using CloudWatch or Athena over DLQ?</p>"}]}