{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"AWS Data Pipeline","text":""},{"location":"index.html#requirements","title":"Requirements","text":"<ul> <li>Parquet files arrive in a S3 bucket every 10 minutes</li> <li>Has fields, id, timestamp and multi level nested JSON data</li> <li>Json field needs to be processed</li> <li>Need Processing state tracking and error handling</li> <li>Auto-scaling</li> <li>Monitoring</li> </ul> <p>Click on a tile to change the primary color:</p> <code>red</code> <code>pink</code> <code>purple</code> <code>deep purple</code> <code>indigo</code> <code>blue</code> <code>light blue</code> <code>cyan</code> <code>teal</code> <code>green</code> <code>light green</code> <code>lime</code> <code>yellow</code> <code>amber</code> <code>orange</code> <code>deep orange</code> <code>brown</code> <code>grey</code> <code>blue grey</code> <code>black</code> <code>white</code>"},{"location":"Architecture/architecture.html","title":"Overview","text":"<p>Parquet files are written every 10 minutes to S3. We will review the options for a scalable, reliable and event driven architecture on AWS. Workflow can optionally be driven by step functions.(Step functions are charged based on number of state transitions)</p>"},{"location":"Architecture/architecture.html#aws-lambda","title":"AWS Lambda","text":"<p>S3 \u2192 EventBridge/SQS \u2192 Lambda \u2192 DynamoDB (state) \u2192 Target System</p>"},{"location":"Architecture/architecture.html#aws-glue","title":"AWS Glue","text":"<p>S3 \u2192 Glue Crawler \u2192 Glue Catalog \u2192 Glue Job \u2192 Processed Data</p>"},{"location":"Architecture/architecture.html#aws-athena","title":"AWS Athena","text":"<p>S3 \u2192 Athena \u2192 Target System</p>"},{"location":"Architecture/athena.html","title":"AWS Athena","text":"<p>It is included here for completeness.</p>"},{"location":"Architecture/athena.html#features","title":"Features","text":"<ul> <li>Query-based: SQL interface for JSON processing</li> <li>Serverless: No infrastructure management</li> <li>Cost-effective: Pay per query</li> <li>JSON functions: Built-in JSON parsing capabilities</li> </ul>"},{"location":"Architecture/athena.html#limitations","title":"Limitations","text":"<ul> <li>Athena is query-oriented, not processing-oriented.</li> <li>No built-in scheduling or event-driven processing</li> </ul>"},{"location":"Architecture/awslambda.html","title":"AWS Lambda","text":""},{"location":"Architecture/awslambda.html#workflow","title":"Workflow","text":"<ul> <li>Trigger a Lambda function when files arrive in S3 bucket</li> <li>Read and process parquet file using pyarrow/fastparquet</li> <li>write status and error logs     <ul> <li>Track status in DynamoDB. Both success and failure</li> <li>Use Dead letter Queue SQS</li> </ul> </li> <li>Step Functions for orchesteration(Optional)</li> </ul>"},{"location":"Architecture/awslambda.html#pros","title":"Pros","text":"<ul> <li>Event-driven, processes files immediately</li> <li>Automatic scaling with concurrent executions</li> <li>Cost-effective for sporadic workloads</li> <li>Low operational overhead</li> </ul>"},{"location":"Architecture/awslambda.html#cons","title":"Cons","text":"<ul> <li>15-minute timeout limit</li> <li>10GB memory limit</li> <li>512MB ephemeral storage (can extend to 10GB)</li> <li>Cold start latency unless provisioned</li> <li>Step functions cost is based on number of transitions</li> </ul>"},{"location":"Architecture/awslambda.html#decoupling","title":"Decoupling","text":"<p>Use SQS for a decoupled design. S3 triggers an Eventbridge rule with SQS as target. SQS has builtin retry logic and backpressure tolerance.</p>"},{"location":"Architecture/glue.html","title":"AWS Glue","text":"<p>AWS Glue is more suited for ETL and Integration. It can run in Python Shell mode or Spark. A Glue Job can be triggered using EventBridge.</p>"},{"location":"Architecture/glue.html#features","title":"Features","text":"<ul> <li>Glue Crawler to catalog data</li> <li>Glue ETL jobs (Spark-based)</li> <li>Bookmarking for tracking processed files</li> <li>Glue Data Quality for validation</li> <li>CloudWatch for monitoring</li> </ul>"},{"location":"Architecture/glue.html#pros","title":"Pros","text":"<ul> <li>Built-in bookmarking for state tracking</li> <li>Handles large files efficiently</li> <li>Auto-scaling with DPUs</li> <li>Rich ETL capabilities</li> <li>Supports streaming with Glue Streaming</li> </ul>"},{"location":"Architecture/glue.html#cons","title":"Cons:","text":"<ul> <li>Higher minimum cost (10-minute billing minimum)</li> <li>Spark overhead for small files</li> <li>More complex setup</li> </ul>"},{"location":"Architecture/monitoring.html","title":"Monitoring","text":"<ul> <li>Cloudwatch Logs for Lambda function</li> <li>Cloudwatch Metrics and Alarms</li> <li>DynamoDB streams for realtime tracking</li> </ul>"},{"location":"Implementation/BatchProcess.html","title":"Batch Process Parquet with Monitoring","text":"<pre><code>import pyarrow as pa\nimport pyarrow.parquet as pq\nimport pandas as pd\nimport json\nfrom datetime import datetime\nimport io\nimport psutil\nimport time\nimport threading\nfrom collections import deque\nimport os\n</code></pre>"},{"location":"Implementation/BatchProcess.html#class-for-system-resource-monitoring","title":"Class for system resource monitoring","text":"<pre><code>class ResourceMonitor:\n    \"\"\"Monitor CPU and memory usage during processing\"\"\"\n\n    def __init__(self):\n        self.cpu_samples = deque(maxlen=1000)\n        self.memory_samples = deque(maxlen=1000)\n        self.monitoring = False\n        self.monitor_thread = None\n        self.process = psutil.Process(os.getpid())\n\n    def start_monitoring(self):\n        \"\"\"Start monitoring in a separate thread\"\"\"\n        self.monitoring = True\n        self.monitor_thread = threading.Thread(target=self._monitor_resources)\n        self.monitor_thread.start()\n\n    def stop_monitoring(self):\n        \"\"\"Stop monitoring and return statistics\"\"\"\n        self.monitoring = False\n        if self.monitor_thread:\n            self.monitor_thread.join()\n\n        if not self.cpu_samples or not self.memory_samples:\n            return {}\n\n        cpu_stats = {\n            'avg': sum(self.cpu_samples) / len(self.cpu_samples),\n            'max': max(self.cpu_samples),\n            'min': min(self.cpu_samples)\n        }\n\n        memory_stats = {\n            'avg_mb': sum(self.memory_samples) / len(self.memory_samples),\n            'max_mb': max(self.memory_samples),\n            'min_mb': min(self.memory_samples)\n        }\n\n        return {\n            'cpu_percent': cpu_stats,\n            'memory_mb': memory_stats,\n            'samples_collected': len(self.cpu_samples)\n        }\n\n    def _monitor_resources(self):\n        \"\"\"Monitor resources in background\"\"\"\n        while self.monitoring:\n            # CPU usage (percentage)\n            cpu_percent = self.process.cpu_percent(interval=0.1)\n            self.cpu_samples.append(cpu_percent)\n\n            # Memory usage (MB)\n            memory_info = self.process.memory_info()\n            memory_mb = memory_info.rss / 1024 / 1024\n            self.memory_samples.append(memory_mb)\n\n            time.sleep(0.1)  # Sample every 100ms\n\ndef get_system_info():\n    \"\"\"Get system information\"\"\"\n    return {\n        'cpu_count': psutil.cpu_count(),\n        'cpu_count_logical': psutil.cpu_count(logical=True),\n        'total_memory_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,\n        'available_memory_gb': psutil.virtual_memory().available / 1024 / 1024 / 1024\n    }\n</code></pre>"},{"location":"Implementation/BatchProcess.html#read-in-batches-and-get-system-metrics","title":"Read in batches and get system metrics","text":"<pre><code>def batchread_prq(parquet_file, batch_size=2, monitor=None):\n    \"\"\"\n    reading Parquet file in batches for memory efficiency\n    monitoring can be enable to get cpu and mem usage\n    \"\"\"\n    print(f\"\\nReading Parquet file in batches of {batch_size}:\")\n\n    start_time = time.time()\n    if monitor:\n        monitor.start_monitoring()\n\n    try:\n        parquet_file_obj = pq.ParquetFile(parquet_file)\n\n        batch_num = 0\n        total_rows_processed = 0\n\n        for batch in parquet_file_obj.iter_batches(batch_size=batch_size):\n            batch_num += 1\n            df_batch = batch.to_pandas()\n            total_rows_processed += len(df_batch)\n\n            print(f\"\\nBatch {batch_num}:\")\n            print(f\"  Rows: {len(df_batch)}\")\n            print(f\"  IDs: {df_batch['id'].tolist()}\")\n\n            # Process JSON in this batch\n            for idx, row in df_batch.iterrows():\n                json_data = json.loads(row['attrs'])\n                print(f\"  - {json_data['user']['name']}: {json_data['metadata']['source']}\")\n\n        processing_time = time.time() - start_time\n\n        if monitor:\n            stats = monitor.stop_monitoring()\n            print(f\"\\n\ud83d\udcca Batch Processing Resource Usage:\")\n            print(f\"  Total rows processed: {total_rows_processed}\")\n            print(f\"  Processing time: {processing_time:.2f} seconds\")\n            print(f\"  Rows per second: {total_rows_processed/processing_time:.1f}\")\n            print(f\"  CPU Usage: avg={stats['cpu_percent']['avg']:.1f}%, \"\n                  f\"max={stats['cpu_percent']['max']:.1f}%\")\n            print(f\"  Memory Usage: avg={stats['memory_mb']['avg_mb']:.1f}MB, \"\n                  f\"max={stats['memory_mb']['max_mb']:.1f}MB\")\n\n    finally:\n        if monitor and monitor.monitoring:\n            monitor.stop_monitoring()\n</code></pre> <pre><code>monitor = ResourceMonitor()\nprint(\"\\n\ud83d\udcca System Information:\")\nsys_info = get_system_info()\nfor key, value in sys_info.items():\n    print(f\"  {key}: {value:.2f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n</code></pre> <pre><code>\ud83d\udcca System Information:\n  cpu_count: 12\n  cpu_count_logical: 12\n  total_memory_gb: 32.00\n  available_memory_gb: 15.69\n</code></pre> <pre><code>print(\"\\n\\nTesting batch reading with different batch sizes...\")\nfor batch_size in [100, 500, 1000]:\n    print(f\"\\n--- Batch size: {batch_size} ---\")\n    monitor = ResourceMonitor()\n    read_parquet_in_batches('sample10k.parquet', batch_size=batch_size, monitor=monitor)\n</code></pre>"},{"location":"Implementation/ErrorHandling.html","title":"Error Handling","text":"<p>Transient errors are handled by a retry.</p> <p>If Lambda fails after max retries (default is 2), the event is sent to a DLQ (SQS).</p>"},{"location":"Implementation/ErrorHandling.html#workflow","title":"Workflow","text":"<p>Lambda processes Parquet files from S3.</p> <p>If Lambda fails after max retries (default is 2), the event is sent to a DLQ (SQS).</p> <p>You can inspect or reprocess messages in the DLQ manually or automatically.</p>"},{"location":"Implementation/ErrorHandling.html#setup","title":"Setup","text":"<ol> <li> <p>Create an SQS Queue for DLQ You can do this via the AWS Console or CLI: aws sqs create-queue --queue-name parquet-processing-dlq</p> </li> <li> <p>Attach DLQ to Lambda If using the AWS CLI:</p> </li> </ol> <p>aws lambda update-function-configuration \\   --function-name parquetProcessor \\   --dead-letter-config TargetArn=arn:aws:sqs:us-east-1:123456789012:parquet-processing-dlq If using the AWS Console:</p> <p>Go to your Lambda function</p> <p>Configuration \u2192 Asynchronous Invocation \u2192 DLQ</p> <p>Select the SQS queue</p> <p>Make sure the Lambda function has sqs:SendMessage permission for this DLQ.</p> <ol> <li>IAM Policy for Lambda to Send to DLQ Attach this IAM permission to the Lambda role:</li> </ol> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": \"sqs:SendMessage\",\n  \"Resource\": \"arn:aws:sqs:us-east-1:123456789012:parquet-processing-dlq\"\n}\n</code></pre> <p>Sample Python Lambda Code with Error</p> <pre><code>import boto3\nimport pyarrow.parquet as pq\nimport io\n\ndef lambda_handler(event, context):\n    s3 = boto3.client('s3')\n\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = record['s3']['object']['key']\n\n        try:\n            obj = s3.get_object(Bucket=bucket, Key=key)\n            table = pq.read_table(source=io.BytesIO(obj['Body'].read()))\n            # Process table...\n            print(f\"Processed file: {key}\")\n        except Exception as e:\n            print(f\"Error processing file {key}: {str(e)}\")\n            raise e  # Rethrow to trigger retry and potentially DLQ\n</code></pre> <ol> <li>View Messages in DLQ (Optional Reprocessing)</li> </ol> <pre><code>aws sqs receive-message --queue-url https://sqs.us-east-1.amazonaws.com/123456789012/parquet-processing-dlq\n</code></pre> <p>\ud83d\udd0d Notes DLQ only captures asynchronous invocations (e.g., from S3, SQS, EventBridge). DLQ stores the original event payload that failed.</p>"},{"location":"Implementation/JSONProcessing.html","title":"JSON Processing","text":"<p>For processing JSON, there are 2 options. Streaming and Full load.</p> <p>Python ijson uses streaming.</p> <p>If JSON is present as a column in a parquet file, it can be treated as line-delimited json. Memory may not be an issue in such a case.</p> <p>pyarrow can be considered for reading parquet and its json field.</p>"},{"location":"Implementation/JSONProcessing.html#py-arrow-parquet","title":"py arrow parquet","text":"<pre><code>import ijson\nimport pyarrow.parquet as pq\n\ndef process_parquet_streaming(s3_key):\n    # Read parquet file\n    table = pq.read_table(f's3://{bucket}/{s3_key}')\n\n    # Process each row's JSON column\n    for batch in table.to_batches():\n        for row in batch.to_pandas().itertuples():\n            # Stream parse JSON without loading full structure\n            json_data = row.metadata_column\n\n            # Use ijson for streaming parsing\n            parser = ijson.parse(json_data)\n            for prefix, event, value in parser:\n                # Process JSON incrementally\n                process_json_element(prefix, event, value, row.id, row.timestamp)\n</code></pre>"},{"location":"Implementation/JSONProcessing.html#nested-json","title":"Nested JSON","text":"<pre><code>import ijson\nimport io\nimport json\nfrom typing import Dict, Any\n\ndef parse_nested_json_streaming(parquet_path: str, json_column: str):\n    \"\"\"\n    Parse a Parquet file with streaming JSON parsing for a nested JSON column.\n\n    Args:\n        parquet_path: Path to the Parquet file\n        json_column: Name of the column containing nested JSON data\n    \"\"\"\n    # Open the Parquet file\n    parquet_file = pq.ParquetFile(parquet_path)\n\n    # Get the batches (streaming approach)\n    for batch in parquet_file.iter_batches():\n        # Convert batch to pandas (for easy access)\n        df = batch.to_pandas()\n\n        # Process each row\n        for index, row in df.iterrows():\n            json_str = row[json_column]\n\n            # Skip null/empty values\n            if not json_str or pd.isna(json_str):\n                continue\n\n            # Create a file-like object for ijson\n            json_bytes = json_str.encode('utf-8') if isinstance(json_str, str) else json_str\n            json_io = io.BytesIO(json_bytes)\n\n            # Use ijson for streaming parse\n            try:\n                # Example: Parse the JSON stream for specific elements\n                parser = ijson.parse(json_io)\n\n                # This is where you'd implement your specific parsing logic\n                # Here's an example that just collects paths and values\n                for prefix, event, value in parser:\n                    print(f\"Path: {prefix}, Event: {event}, Value: {value}\")\n\n                # Alternative: Extract specific nested values\n                json_io.seek(0)  # Reset for new parse\n                nested_value = ijson.items(json_io, 'some.nested.path')\n                for item in nested_value:\n                    print(f\"Nested value: {item}\")\n\n            except Exception as e:\n                print(f\"Error parsing JSON in row {index}: {str(e)}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    parse_nested_json_streaming('data.parquet', 'json_data_column')\n</code></pre>"},{"location":"Implementation/dynamodb.html","title":"DynamoDB","text":"<p>Partition Key: file_path Attributes: processing_status, timestamp, error_message, retry_count</p>"},{"location":"Implementation/parseparquet.html","title":"Parse Parquet","text":"<pre><code>import pyarrow as pa\nimport pyarrow.parquet as pq\nimport pandas as pd\nimport json\nfrom datetime import datetime\nimport io\n</code></pre> <p>Store JSON as String</p> <pre><code>def csv_to_parquet_with_schema(csv_file, parquet_file):\n    \"\"\"\n    Convert CSV to Parquet with proper schema definition\n    JSON will be treated as a string field\n    \"\"\"\n\n    df = pd.read_csv(csv_file)\n\n    # Convert create_dt to timestamp\n    df['create_dt'] = pd.to_datetime(df['create_dt'])\n\n    # Define the schema\n    schema = pa.schema([\n        ('id', pa.int64()),\n        ('create_dt', pa.timestamp('us')),\n        ('attrs', pa.string())  # JSON stored as string\n    ])\n\n    # Create PyArrow table\n    table = pa.Table.from_pandas(df, schema=schema)\n\n    # Write to Parquet with compression. Parquet version can also be set\n    pq.write_table(\n        table, \n        parquet_file,\n        compression='snappy',\n        use_dictionary=True       \n    )\n\n    print(f\"Parquet file created: {parquet_file}\")\n\n    # Print file metadata\n    parquet_file_obj = pq.ParquetFile(parquet_file)\n    print(f\"\\nParquet file metadata:\")\n    print(f\"  Number of rows: {parquet_file_obj.metadata.num_rows}\")\n    print(f\"  Number of columns: {parquet_file_obj.metadata.num_columns}\")\n    print(f\"  Schema: {parquet_file_obj.schema}\")\n    print(f\"  Compression: snappy\")\n</code></pre> <pre><code>csv_to_parquet_with_schema('sample_data.csv', 'output_with_json_string.parquet')\n</code></pre> <pre><code>Parquet file created: output_with_json_string.parquet\n\nParquet file metadata:\n  Number of rows: 5\n  Number of columns: 3\n  Schema: &lt;pyarrow._parquet.ParquetSchema object at 0x10e35d580&gt;\nrequired group field_id=-1 schema {\n  optional int64 field_id=-1 id;\n  optional int64 field_id=-1 create_dt (Timestamp(isAdjustedToUTC=false, timeUnit=microseconds, is_from_converted_type=false, force_set_converted_type=false));\n  optional binary field_id=-1 attrs (String);\n}\n\n  Compression: snappy\n</code></pre> <pre><code>def read_and_parse_parquet(parquet_file):\n    \"\"\"\n    Read Parquet file and demonstrate parsing JSON column\n    \"\"\"\n    print(f\"\\nReading Parquet file: {parquet_file}\")\n\n    # Read the entire file\n    table = pq.read_table(parquet_file)\n    df = table.to_pandas()\n\n    print(f\"\\nDataFrame shape: {df.shape}\")\n    print(f\"\\nDataFrame dtypes:\\n{df.dtypes}\")\n\n    # Parse JSON column for first row as example\n    print(\"\\nParsing JSON from first row:\")\n    first_row_json = json.loads(df.iloc[0]['attrs'])\n    print(json.dumps(first_row_json, indent=2))\n\n    # Example: Extract specific nested values\n    print(\"\\nExtracting nested values from all rows:\")\n    for idx, row in df.iterrows():\n        json_data = json.loads(row['attrs'])\n        user_name = json_data['user']['name']\n        city = json_data['user']['address']['city']\n        lat = json_data['user']['address']['coordinates']['lat']\n        email_pref = json_data['user']['preferences']['notifications']['email']\n        tags = json_data['metadata']['tags']\n\n        print(f\"ID {row['id']}: {user_name} from {city} (lat: {lat}), \"\n              f\"email notifications: {email_pref}, tags: {tags}\")\n</code></pre> <pre><code>read_and_parse_parquet('output_with_json_string.parquet')\n</code></pre> <pre><code>Reading Parquet file: output_with_json_string.parquet\n\nDataFrame shape: (5, 3)\n\nDataFrame dtypes:\nid                    int64\ncreate_dt    datetime64[us]\nattrs                object\ndtype: object\n\nParsing JSON from first row:\n{\n  \"user\": {\n    \"name\": \"John Doe\",\n    \"age\": 32,\n    \"address\": {\n      \"street\": \"123 Main St\",\n      \"city\": \"New York\",\n      \"state\": \"NY\",\n      \"coordinates\": {\n        \"lat\": 40.7128,\n        \"lng\": -74.006\n      }\n    },\n    \"preferences\": {\n      \"notifications\": {\n        \"email\": true,\n        \"sms\": false,\n        \"push\": true\n      },\n      \"theme\": \"dark\"\n    }\n  },\n  \"metadata\": {\n    \"source\": \"web\",\n    \"version\": \"2.1.0\",\n    \"tags\": [\n      \"premium\",\n      \"verified\",\n      \"active\"\n    ]\n  }\n}\n\nExtracting nested values from all rows:\nID 1: John Doe from New York (lat: 40.7128), email notifications: True, tags: ['premium', 'verified', 'active']\nID 2: Jane Smith from Los Angeles (lat: 34.0522), email notifications: False, tags: ['basic', 'new_user']\nID 3: Bob Johnson from Chicago (lat: 41.8781), email notifications: True, tags: ['enterprise', 'verified', 'beta_tester']\nID 4: Alice Brown from Houston (lat: 29.7604), email notifications: True, tags: ['premium', 'long_term_user', 'vip']\nID 5: Charlie Wilson from Phoenix (lat: 33.4484), email notifications: False, tags: ['basic']\n</code></pre> <pre><code>def read_parquet_in_batches(parquet_file, batch_size=2):\n    \"\"\"\n    Demonstrate reading Parquet file in batches for memory efficiency\n    \"\"\"\n    print(f\"\\nReading Parquet file in batches of {batch_size}:\")\n\n    parquet_file_obj = pq.ParquetFile(parquet_file)\n\n    batch_num = 0\n    for batch in parquet_file_obj.iter_batches(batch_size=batch_size):\n        batch_num += 1\n        df_batch = batch.to_pandas()\n        print(f\"\\nBatch {batch_num}:\")\n        print(f\"  Rows: {len(df_batch)}\")\n        print(f\"  IDs: {df_batch['id'].tolist()}\")\n\n        # Process JSON in this batch\n        for idx, row in df_batch.iterrows():\n            json_data = json.loads(row['attrs'])\n            print(f\"  - {json_data['user']['name']}: {json_data['metadata']['source']}\")\n</code></pre> <pre><code>read_parquet_in_batches('output_with_json_string.parquet')\n</code></pre> <pre><code>Reading Parquet file in batches of 2:\n\nBatch 1:\n  Rows: 2\n  IDs: [1, 2]\n  - John Doe: web\n  - Jane Smith: mobile\n\nBatch 2:\n  Rows: 2\n  IDs: [3, 4]\n  - Bob Johnson: api\n  - Alice Brown: web\n\nBatch 3:\n  Rows: 1\n  IDs: [5]\n  - Charlie Wilson: mobile\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Scripts/dlq.html","title":"Terraform Script","text":"<ul> <li>Creates an S3 bucket (for Parquet uploads). Name shown in output</li> <li>Sets up an S3 Event Notification to trigger the Lambda</li> <li>Creates an SQS queue (as a Dead Letter Queue)</li> <li>Creates a Lambda function (that processes Parquet files)</li> </ul> <p>terraform/main.tf terraform/variables.tf terraform/lambda/lambda_function.py</p> <p><pre><code>variables.tf\nvariable \"region\" {\n  default = \"us-east-1\"\n}\n\nmain.tf\nprovider \"aws\" {\n  region = var.region\n}\n\nresource \"aws_s3_bucket\" \"parquet_bucket\" {\n  bucket = \"parquet-ingest-${random_id.suffix.hex}\"\n}\n\nresource \"random_id\" \"suffix\" {\n  byte_length = 4\n}\n\nresource \"aws_sqs_queue\" \"dlq\" {\n  name = \"parquet-processing-dlq\"\n}\n\nresource \"aws_iam_role\" \"lambda_exec_role\" {\n  name = \"lambda_parquet_exec_role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\",\n    Statement = [{\n      Action = \"sts:AssumeRole\",\n      Effect = \"Allow\",\n      Principal = {\n        Service = \"lambda.amazonaws.com\"\n      }\n    }]\n  })\n}\n\nresource \"aws_iam_policy\" \"lambda_policy\" {\n  name = \"lambda_parquet_policy\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\",\n    Statement = [\n      {\n        Effect = \"Allow\",\n        Action = [\n          \"s3:GetObject\",\n          \"s3:ListBucket\"\n        ],\n        Resource = [\n          aws_s3_bucket.parquet_bucket.arn,\n          \"${aws_s3_bucket.parquet_bucket.arn}/*\"\n        ]\n      },\n      {\n        Effect = \"Allow\",\n        Action = \"logs:*\",\n        Resource = \"*\"\n      },\n      {\n        Effect = \"Allow\",\n        Action = \"sqs:SendMessage\",\n        Resource = aws_sqs_queue.dlq.arn\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"attach_policy\" {\n  role       = aws_iam_role.lambda_exec_role.name\n  policy_arn = aws_iam_policy.lambda_policy.arn\n}\n\ndata \"archive_file\" \"lambda_zip\" {\n  type        = \"zip\"\n  source_dir  = \"${path.module}/lambda\"\n  output_path = \"${path.module}/lambda.zip\"\n}\n\nresource \"aws_lambda_function\" \"parquet_processor\" {\n  function_name = \"parquetProcessor\"\n  filename      = data.archive_file.lambda_zip.output_path\n  source_code_hash = data.archive_file.lambda_zip.output_base64sha256\n  role          = aws_iam_role.lambda_exec_role.arn\n  handler       = \"lambda_function.lambda_handler\"\n  runtime       = \"python3.11\"\n  timeout       = 30\n\n  dead_letter_config {\n    target_arn = aws_sqs_queue.dlq.arn\n  }\n}\n\nresource \"aws_s3_bucket_notification\" \"bucket_notify\" {\n  bucket = aws_s3_bucket.parquet_bucket.id\n\n  lambda_function {\n    lambda_function_arn = aws_lambda_function.parquet_processor.arn\n    events              = [\"s3:ObjectCreated:*\"]\n  }\n\n  depends_on = [aws_lambda_permission.allow_s3]\n}\n\nresource \"aws_lambda_permission\" \"allow_s3\" {\n  statement_id  = \"AllowExecutionFromS3\"\n  action        = \"lambda:InvokeFunction\"\n  function_name = aws_lambda_function.parquet_processor.function_name\n  principal     = \"s3.amazonaws.com\"\n  source_arn    = aws_s3_bucket.parquet_bucket.arn\n}\n</code></pre> Step 3: lambda/lambda_function.py</p> <p><pre><code>import boto3\nimport pyarrow.parquet as pq\nimport io\n\ndef lambda_handler(event, context):\n    s3 = boto3.client('s3')\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = record['s3']['object']['key']\n        try:\n            obj = s3.get_object(Bucket=bucket, Key=key)\n            table = pq.read_table(source=io.BytesIO(obj['Body'].read()))\n            print(f\"Processed file: {key}\")\n        except Exception as e:\n            print(f\"ERROR processing {key}: {e}\")\n            raise e  # Ensure Lambda fails to trigger DLQ\n</code></pre> terraform init terraform apply</p>"}]}